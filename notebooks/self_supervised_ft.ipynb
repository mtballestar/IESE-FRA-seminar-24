{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cy3BkxDLnNNC"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "Perform additional pre-training on a language model (we will use DistilBERT) through\n",
        "the Masked Language Modeling (MLM) task. The pre-trained model will be stored\n",
        "in: ./models/pre-train/final/\n",
        "\n",
        "Some additional resources:\n",
        "- https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling\n",
        "- https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
        "- https://www.thepythoncode.com/article/pretraining-bert-huggingface-transformers-in-python\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#%%\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "from transformers import DistilBertTokenizer, DistilBertForMaskedLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        "import yaml\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# load utils functions\n",
        "sys.path.insert(0, \"../utils/\")\n",
        "import nlp_utils as nlp_utils\n",
        "\n",
        "\n",
        "print(f\"GPU: {torch.cuda.is_available()}\")\n",
        "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "\n",
        "data_path = \"../data/\"\n",
        "hand_path = \"./\"\n",
        "\n",
        "output_path = \"../models/pre-train/\"\n",
        "checkpoint_path = \"../models/pre-train/checkpoints/\"\n",
        "final_model_path = \"../models/pre-train/final/\"\n",
        "\n",
        "# check if the output directories exist and, if not, create them\n",
        "for dir_path in [output_path, checkpoint_path, final_model_path]:\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path)\n",
        "        print(f\"Directory {dir_path} created successfully\")\n",
        "    else:\n",
        "        print(f\"Directory {dir_path} already exists\")\n",
        "\n",
        "# open YAML file with parameters for data consolidation\n",
        "stream = open(hand_path + \"distilbert_params.yaml\", 'r')\n",
        "params = yaml.load(stream, Loader=yaml.Loader)\n",
        "\n",
        "# %%\n",
        "\n",
        "#=============================\n",
        "# 0. Consolidate data\n",
        "#=============================\n",
        "\n",
        "# unpack params from the yaml file\n",
        "test_frac = params[\"test_frac\"]\n",
        "seed = params[\"seed\"]\n",
        "\n",
        "# join all datasets together and shuffle the examples\n",
        "df = pd.read_parquet(\"DATA\")\n",
        "df = df[[\"sequence_id\", \"sequence\"]]\n",
        "df = df.sample(frac=1.0)\n",
        "\n",
        "# transform into Dataset class\n",
        "raw_datasets = Dataset.from_pandas(df)\n",
        "\n",
        "# split into train/test\n",
        "raw_datasets = raw_datasets.train_test_split(test_size=test_frac, seed=seed)\n",
        "\n",
        "# clean memory\n",
        "del df\n",
        "print(f\"Sequences for training: {len(raw_datasets['train'])}\")\n",
        "print(f\"Sequences for testing: {len(raw_datasets['test'])}\")\n",
        "\n",
        "#%%\n",
        "\n",
        "#=============================\n",
        "# 1. Load model and tokenizer\n",
        "#=============================\n",
        "\n",
        "# tokenize text\n",
        "model_name = params[\"model_name\"]\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "model = DistilBertForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "# if we want to get more details about the model\n",
        "# print(model.config)\n",
        "#%%\n",
        "\n",
        "#=============================\n",
        "# 2. Tokenization\n",
        "#=============================\n",
        "\n",
        "print(f\"Initial vocab size: {len(tokenizer)}\")\n",
        "padding = True\n",
        "max_sent_size = params[\"max_sent_size\"]\n",
        "\n",
        "# add new tokens to the tokenizer\n",
        "tokenizer.add_tokens(\"\\n\")\n",
        "print(f\"Final vocab size: {len(tokenizer)}\")\n",
        "# add random word embeddings to new tokens in the model\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# verify that new line gets a token\n",
        "print(tokenizer(\"\\n \\n\"))\n",
        "\n",
        "# apply tokenization to the complete dataset\n",
        "start_token = time.perf_counter()\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    nlp_utils.tokenize_function,\n",
        "    fn_kwargs={\"tokenizer\": tokenizer,\n",
        "               \"col_text\": \"sequence\",\n",
        "               \"max_sent_size\": max_sent_size},\n",
        "    batched=True,\n",
        "    #num_proc=32, # num vCPUs\n",
        "    #remove_columns=[\"frag\"],\n",
        "    #desc=\"Running tokenizer on dataset line_by_line\",\n",
        ")\n",
        "\n",
        "del raw_datasets\n",
        "duration_token = (time.perf_counter() - start_token)/60\n",
        "print(f\"Tokenization ready in: {duration_token} minutes\")\n",
        "#%%\n",
        "\n",
        "#=============================\n",
        "# 3. Prepare Masking\n",
        "#=============================\n",
        "\n",
        "# initialize collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm_probability= params[\"mlm_probability\"], # probability of replacing a token by [MASK]\n",
        "    pad_to_multiple_of=8 if params[\"pad_to_multiple_of_8\"] else None,\n",
        ")\n",
        "\n",
        "# data will only get \"collated\" as the model trains\n",
        "print(data_collator)\n",
        "#%%\n",
        "\n",
        "#=============================\n",
        "# 4. Training\n",
        "#=============================\n",
        "\n",
        "# consolidate training arguments\n",
        "learning_rate = float(params[\"learning_rate\"])\n",
        "batch_size = int(params[\"batch_size\"])\n",
        "\n",
        "training_args = TrainingArguments(checkpoint_path,\n",
        "                                learning_rate=learning_rate,\n",
        "                                per_device_train_batch_size=batch_size,\n",
        "                                per_device_eval_batch_size=batch_size ,\n",
        "                                max_steps=params[\"steps\"],\n",
        "                                warmup_ratio=params[\"warmup_ratio\"],\n",
        "                                evaluation_strategy=\"steps\",\n",
        "                                eval_steps= params[\"eval_steps\"],\n",
        "                                save_strategy=\"no\",\n",
        "                                #save_steps=25000,\n",
        "                                #save_total_limit=1,\n",
        "                                logging_dir=checkpoint_path,\n",
        "                                logging_strategy=\"steps\",\n",
        "                                logging_steps=params[\"logging_steps\"]\n",
        "                                )\n",
        "\n",
        "# Initialize our Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    tokenizer=tokenizer,    # use if we want to save the tokenizer as well\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "#%%\n",
        "\n",
        "# TRAIN!\n",
        "start_train = time.perf_counter()\n",
        "checkpoint = None\n",
        "\n",
        "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
        "train_metrics = train_result.metrics\n",
        "\n",
        "duration_train = (time.perf_counter() - start_train)/60\n",
        "print(f\"Training finished in: {duration_train} minutes\")\n",
        "\n",
        "# save final model\n",
        "trainer.save_model(final_model_path)\n",
        "# %%"
      ]
    }
  ]
}